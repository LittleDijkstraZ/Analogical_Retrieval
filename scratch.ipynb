{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dijkstraz/anaconda3/envs/colab/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0, './analogies_mining')\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from functools import partial\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model_general(model_name):\n",
    "#     from transformers import AutoTokenizer, pipeline\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "#     pipe = pipeline(\n",
    "#         \"text-generation\",\n",
    "#         model=model_name,\n",
    "#         tokenizer=tokenizer,\n",
    "#         torch_dtype=torch.float16,\n",
    "#         trust_remote_code=True,\n",
    "#         device_map=\"cuda:0\",\n",
    "#         # max_new_tokens=8,\n",
    "#         # do_sample=True,\n",
    "\n",
    "        \n",
    "#     )\n",
    "#     tokenizer = pipe.tokenizer\n",
    "#     model = pipe.model\n",
    "#     model.eval()\n",
    "#     return model, tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "from accelerate.utils import BnbQuantizationConfig\n",
    "\n",
    "def get_model_general(model_name):\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"fp4\",\n",
    "    )\n",
    "    # quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        padding_side=\"left\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # pipe = pipeline(\n",
    "    #     \"text-generation\",\n",
    "    #     model=model_name,\n",
    "    #     tokenizer=tokenizer,\n",
    "    #     trust_remote_code=True,\n",
    "    #     device_map=\"cuda:0\",\n",
    "    #     quantization_config=quantization_config,  # Pass the quantization config\n",
    "    #     load_in_4bit=True,                     # Enable 4-bit quantization\n",
    "\n",
    "    # )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"cuda:0\")\n",
    "    model.eval()\n",
    "    # tokenizer = pipe.tokenizer\n",
    "    # model = pipe.model\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.2.2. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "/home/dijkstraz/anaconda3/envs/colab/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading multi-qa-mpnet-base-dot-v1\n",
      "24432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6612cc42d59f4808b9cf5773e24340f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5650a8772a884b2bbc59156bcc3b63e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97728, 768]) torch.Size([97728, 768])\n",
      "cossim\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf2626af7af4c38a481d2e0494996fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting\n",
      "sorting finished\n",
      "Precision@1: 0.5243\n",
      "24432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de4dfeec8b44b6bac3416545a057b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98831cd1a8a945bca7e843e7ed8b58ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97728, 768]) torch.Size([97728, 768])\n",
      "cossim\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc087857c38417f8b713ce1c1f2d9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting\n",
      "sorting finished\n",
      "Precision@1: 0.1999\n",
      "24432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1525166638044b26b4e478c31070fa1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f579a4642146738b3f632207bfc326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97728, 768]) torch.Size([97728, 768])\n",
      "cossim\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5c0ce4fc9a4b80ac0b5a623e68cfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting\n",
      "sorting finished\n",
      "Precision@1: 0.2593\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'multi-qa-mpnet-base-dot-v1'\n",
    "# model_name = 'paraphrase-MiniLM-L12-v2'\n",
    "\n",
    "# model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_options(options_str):\n",
    "    splits = ['A.', 'B.', 'C.', 'D.']\n",
    "    final_options = []\n",
    "    for sidx in range(len(splits)):\n",
    "        if sidx == len(splits) - 1:\n",
    "            x = options_str.split(splits[sidx])[1]\n",
    "        else:\n",
    "            x = options_str.split(splits[sidx])[1].split(splits[sidx+1])[0]\n",
    "        final_options.append(x)\n",
    "    return final_options\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def predict_labels(questions, options, model, bs=256):\n",
    "    question_embeddings = model.encode(questions, show_progress_bar=True, batch_size=bs, convert_to_tensor=True)\n",
    "    question_embeddings = question_embeddings.reshape(len(questions), -1)\n",
    "    # question_embeddings = np.repeat(question_embeddings, 4, axis=0)\n",
    "    question_embeddings = torch.repeat_interleave(question_embeddings, 4, dim=0)\n",
    "    option_embeddings = model.encode(options, show_progress_bar=True, batch_size=bs, convert_to_tensor=True)\n",
    "    print(question_embeddings.shape, option_embeddings.shape)\n",
    "\n",
    "\n",
    "    similarities = []\n",
    "    print('cossim')\n",
    "    for qidx in tqdm(range(0, len(question_embeddings), bs)):\n",
    "        sim = torch.nn.functional.cosine_similarity(question_embeddings[qidx:qidx+bs], option_embeddings[qidx:qidx+bs], dim=-1)\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    similarities = torch.hstack(similarities)\n",
    "    similarities = similarities.reshape(len(questions), 4)\n",
    "    print('sorting')\n",
    "    # ranked_indices = torch.argsort(similarities, dim=-1, descending=True)\n",
    "    predicted_labels = torch.argmax(similarities, dim=-1).cpu().numpy()\n",
    "    print('sorting finished')\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "\n",
    "def predict_labels_FollowIR(quesitons, options, model, tokenizer, bs=32):\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    token_false_id = tokenizer.get_vocab()[\"false\"]\n",
    "    token_true_id = tokenizer.get_vocab()[\"true\"]\n",
    "    template = \"\"\"<s> [INST] Consider the Document to be relevant only if it can be analogous to the Query. Answer in (true/false)\n",
    "\n",
    "    Query: {query}\n",
    "    Document: {text}\n",
    "    Analogically Relevant (only output one word, either \"true\" or \"false\"):  [/INST] \"\"\"\n",
    "\n",
    "    # assert bs % 4 == 0, \"Batch size must be a multiple of 4\"\n",
    "    scores = []\n",
    "    for bsidx in tqdm(range(0, len(options), bs)):\n",
    "        cur_options = options[bsidx:bsidx+bs]\n",
    "        cur_questions = [quesitons[opidx//4] for opidx in range(bsidx, bsidx+bs)]\n",
    "\n",
    "        prompts = [\n",
    "            template.format(query=query, text=text) for (query, text) in zip(cur_questions, cur_options)\n",
    "        ]\n",
    "        tokens = tokenizer(\n",
    "            prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            pad_to_multiple_of=None,\n",
    "        )\n",
    "\n",
    "        # move to cuda if desired\n",
    "        for key in tokens:\n",
    "            tokens[key] = tokens[key].cuda()\n",
    "\n",
    "        # calculate the scores by comparing true and false tokens\n",
    "        batch_scores = model(**tokens).logits[:, -1, :]\n",
    "        true_vector = batch_scores[:, token_true_id]\n",
    "        false_vector = batch_scores[:, token_false_id]\n",
    "        batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "        batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "        cur_scores = batch_scores[:, 1].exp().tolist()\n",
    "        scores.extend(cur_scores)\n",
    "    print(len(cur_scores))\n",
    "    scores = np.array(scores)\n",
    "    scores = scores.reshape(len(quesitons), 4)\n",
    "    predicted_labels = np.argmax(scores, axis=1)\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "def preprocess_data(samples):\n",
    "    if 'Story' not in samples:\n",
    "        questions_pool = samples['Sentence']\n",
    "    else:\n",
    "        questions_pool = samples['Story']\n",
    "    questions = [sample for sample in questions_pool]\n",
    "    all_options = [sample for sample in samples['Options']]\n",
    "    all_options = [preprocess_options(options) for options in all_options]\n",
    "    flattened_options = [option for options in all_options for option in options]\n",
    "    return questions, flattened_options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_ranking(dataset, pred_func):\n",
    "\n",
    "    questions, options = preprocess_data(dataset)\n",
    "    print(len(questions))\n",
    "    predicted_labels = pred_func(questions, options)\n",
    "    \n",
    "    labels = dataset['Label']  # The index of the correct option\n",
    "    labels = np.array([ord(label) - ord('A') for label in labels])\n",
    "    total_samples = len(labels)    \n",
    "    correct = (predicted_labels == labels)\n",
    "    precision_at_1 = sum(correct) / total_samples\n",
    "\n",
    "    incorrect_sample_indices = []\n",
    "    correct_sample_indices = []\n",
    "    for i, crr in enumerate(correct):\n",
    "        if not crr:\n",
    "            incorrect_sample_indices.append(i)\n",
    "        else:\n",
    "            correct_sample_indices.append(i)\n",
    "\n",
    "    return  precision_at_1, incorrect_sample_indices, correct_sample_indices\n",
    "\n",
    "results = {}\n",
    "\n",
    "model_name_list =[\n",
    "    # 'all-mpnet-base-v2', #  0.5481, 0.2312, 0.2559 (1 mins)\n",
    "    # 'sentence-t5-xl', # 0.6953 (13 mins), 0.2556, 0.2564\n",
    "    # 'gtr-t5-xl', # 0.5860,\n",
    "\t'multi-qa-mpnet-base-dot-v1',\n",
    "    # \"jhu-clsp/FollowIR-7B\" #  0.6290\n",
    "]\n",
    "\n",
    "for model_name in model_name_list:\n",
    "    if model_name== \"jhu-clsp/FollowIR-7B\":\n",
    "        # model = AutoModelForCausalLM.from_pretrained(\n",
    "        #     model_name,\n",
    "        #     torch_dtype=torch.float16  # Load in fp16 precision\n",
    "        # ).to('cuda')\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(\n",
    "        #     model_name, padding_side=\"left\"\n",
    "        # )\n",
    "        model, tokenizer = get_model_general(model_name)\n",
    "        bs = 2\n",
    "        pred_func = partial(predict_labels_FollowIR, model=model, tokenizer=tokenizer, bs=bs)\n",
    "    \n",
    "    else:\n",
    "        bs = 256 if not 'xl' in model_name else 64\n",
    "        bs = bs if not 'xxl' in model_name else 32\n",
    "        model = SentenceTransformer(model_name, device='cuda:0')\n",
    "        pred_func = partial(predict_labels, model=model, bs=bs)\n",
    "    print('finish loading', model_name)\n",
    "\n",
    "    results[f'{model_name}'] = [] \n",
    "    # for datastr in ['1', '10', '30']:# '10', '30',\n",
    "    for datastr in ['1', '10', '30']:# '10', '30',\n",
    "    \n",
    "        dataset = load_dataset('jhu-clsp/AnaloBench', f'T1S{datastr}-Full')['train']\n",
    "        precision_at_1, incorrect_sample_indices, correct_sample_indices = evaluate_ranking(dataset, pred_func)\n",
    "        print(f'Precision@1: {precision_at_1:.4f}')\n",
    "        results[f'{model_name}'].append(precision_at_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5243, 0.1999, 0.2593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_example(example_idx):\n",
    "    example = dataset[example_idx]\n",
    "    lines = '='*10\n",
    "    print(lines, f\"Query: {example['Sentence']}\")\n",
    "    print(lines, f\"Options:\")\n",
    "    options = preprocess_options(example['Options'])\n",
    "    for idx, option in enumerate(options):\n",
    "        print(f\"  {chr(ord('A') + idx)}. {option}\")\n",
    "    answer_idx = ord(example['Label']) - ord('A')\n",
    "    print(lines, f\"Correct Answer: {example['Options'][answer_idx]}\")\n",
    "    print(lines, f\"Predicted Answer: \")\n",
    "    print()\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "interact(visualize_example, example_idx=incorrect_sample_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow IR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model loading and setup\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import PeftModel, PeftConfig\n",
    "import numpy as np\n",
    "\n",
    "class Promptriever:\n",
    "    def __init__(self, model_name_or_path):\n",
    "        self.model, self.tokenizer = self.get_model(model_name_or_path)\n",
    "        self.model.eval().cuda()\n",
    "\n",
    "    def get_model(self, peft_model_name):\n",
    "        # Load the PEFT configuration to get the base model name\n",
    "        peft_config = PeftConfig.from_pretrained(peft_model_name)\n",
    "        base_model_name = peft_config.base_model_name_or_path\n",
    "\n",
    "        # Load the base model and tokenizer\n",
    "        base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.padding_side = \"right\"\n",
    "\n",
    "        # Load and merge the PEFT model\n",
    "        model = PeftModel.from_pretrained(base_model, peft_model_name)\n",
    "        model = model.merge_and_unload()\n",
    "\n",
    "        # can be much longer, but for the example 512 is enough\n",
    "        model.config.max_length = 512\n",
    "        tokenizer.model_max_length = 512\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    def create_batch_dict(self, tokenizer, input_texts):\n",
    "        max_length = self.model.config.max_length\n",
    "        batch_dict = tokenizer(\n",
    "            input_texts,\n",
    "            max_length=max_length - 1,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=False,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "        )\n",
    "        batch_dict[\"input_ids\"] = [\n",
    "            input_ids + [tokenizer.eos_token_id]\n",
    "            for input_ids in batch_dict[\"input_ids\"]\n",
    "        ]\n",
    "        return tokenizer.pad(\n",
    "            batch_dict,\n",
    "            padding=True,\n",
    "            pad_to_multiple_of=8,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    def encode(self, sentences, max_length: int = 2048, batch_size: int = 4):\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch_texts = sentences[i : i + batch_size]\n",
    "\n",
    "            batch_dict = self.create_batch_dict(self.tokenizer, batch_texts)\n",
    "            batch_dict = {\n",
    "                key: value.to(self.model.device) for key, value in batch_dict.items()\n",
    "            }\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**batch_dict)\n",
    "                    last_hidden_state = outputs.last_hidden_state\n",
    "                    sequence_lengths = batch_dict[\"attention_mask\"].sum(dim=1) - 1\n",
    "                    batch_size = last_hidden_state.shape[0]\n",
    "                    reps = last_hidden_state[\n",
    "                        torch.arange(batch_size, device=last_hidden_state.device),\n",
    "                        sequence_lengths,\n",
    "                    ]\n",
    "                    embeddings = F.normalize(reps, p=2, dim=-1)\n",
    "                    all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "        return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "# Initialize the model\n",
    "model = Promptriever(\"samaya-ai/promptriever-llama3.1-8b-instruct-v1\")\n",
    "\n",
    "# Example query and instruction\n",
    "query = \"What universities are in Baltimore, Maryland?\"\n",
    "\n",
    "# add specific relevance conditions if desired (and/or/not) and any other prompts\n",
    "instruction = \"A relevant document would describe any university in Baltimore. I am not interested in any university that was the first American university. Think carefully about these conditions when determining relevance.\"\n",
    "\n",
    "# Combine query and instruction with **two spaces** after \"query: \"\n",
    "input_text = f\"query:  {query.strip()} {instruction.strip()}\".strip()\n",
    "\n",
    "# Example documents\n",
    "# NOTE: double space after `passage:`\n",
    "doc1 = \"passage:  Johns Hopkins University (often abbreviated as Johns Hopkins, Hopkins, or JHU) is a private research university in Baltimore, Maryland. Founded in 1876, Johns Hopkins was the first American university based on the European research institution model.\"\n",
    "doc2 = \"passage:  Johns Hopkins University (often abbreviated as Johns Hopkins, Hopkins, or JHU) is a private research university in Baltimore, Maryland. Founded in 1876, Johns Hopkins was the second American university based on the European research institution model.\"\n",
    "\n",
    "# Encode query and documents\n",
    "query_embedding = model.encode([input_text])\n",
    "doc_embeddings = model.encode([doc1, doc2])\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = np.dot(query_embedding, doc_embeddings.T)[0]\n",
    "print(f\"Similarities: {similarities}\") # Similarities: [0.53341305 0.53451955]\n",
    "assert similarities[1] > similarities[0]\n",
    "\n",
    "\n",
    "# change up the instruction to the opposite, to see it works\n",
    "instruction = \"A relevant document would describe any university in Baltimore. I am interested in any university that was the first American university. Think carefully about these conditions when determining relevance.\"\n",
    "input_text = f\"query:  {query.strip()} {instruction.strip()}\".strip()\n",
    "query_embedding = model.encode([input_text])\n",
    "similarities = np.dot(query_embedding, doc_embeddings.T)[0]\n",
    "print(f\"Similarities: {similarities}\") # Similarities: [0.60182875 0.5874183 ]\n",
    "assert similarities[0] > similarities[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import PeftModel, PeftConfig\n",
    "import numpy as np\n",
    "\n",
    "# Include the Promptriever class from your second snippet\n",
    "class Promptriever:\n",
    "    def __init__(self, model_name_or_path):\n",
    "        self.model, self.tokenizer = self.get_model(model_name_or_path)\n",
    "        self.model.eval().cuda()\n",
    "\n",
    "    def get_model(self, peft_model_name):\n",
    "        # Load the PEFT configuration to get the base model name\n",
    "        peft_config = PeftConfig.from_pretrained(peft_model_name)\n",
    "        base_model_name = peft_config.base_model_name_or_path\n",
    "\n",
    "        # Load the base model and tokenizer\n",
    "        base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.padding_side = \"right\"\n",
    "\n",
    "        # Load and merge the PEFT model\n",
    "        model = PeftModel.from_pretrained(base_model, peft_model_name)\n",
    "        model = model.merge_and_unload()\n",
    "\n",
    "        # Set maximum sequence length\n",
    "        model.config.max_length = 512\n",
    "        tokenizer.model_max_length = 512\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    def create_batch_dict(self, tokenizer, input_texts):\n",
    "        max_length = self.model.config.max_length\n",
    "        batch_dict = tokenizer(\n",
    "            input_texts,\n",
    "            max_length=max_length - 1,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=False,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "        )\n",
    "        batch_dict[\"input_ids\"] = [\n",
    "            input_ids + [tokenizer.eos_token_id]\n",
    "            for input_ids in batch_dict[\"input_ids\"]\n",
    "        ]\n",
    "        return tokenizer.pad(\n",
    "            batch_dict,\n",
    "            padding=True,\n",
    "            pad_to_multiple_of=8,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    def encode(self, sentences, max_length: int = 2048, batch_size: int = 4):\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch_texts = sentences[i : i + batch_size]\n",
    "\n",
    "            batch_dict = self.create_batch_dict(self.tokenizer, batch_texts)\n",
    "            batch_dict = {\n",
    "                key: value.to(self.model.device) for key, value in batch_dict.items()\n",
    "            }\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**batch_dict)\n",
    "                    last_hidden_state = outputs.last_hidden_state\n",
    "                    sequence_lengths = batch_dict[\"attention_mask\"].sum(dim=1) - 1\n",
    "                    batch_size_local = last_hidden_state.shape[0]\n",
    "                    reps = last_hidden_state[\n",
    "                        torch.arange(batch_size_local, device=last_hidden_state.device),\n",
    "                        sequence_lengths,\n",
    "                    ]\n",
    "                    embeddings = F.normalize(reps, p=2, dim=-1)\n",
    "                    all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "        return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "# Initialize the Promptriever model\n",
    "promptriever_model = Promptriever(\"samaya-ai/promptriever-llama3.1-8b-instruct-v1\")\n",
    "\n",
    "# Modified predict_labels_FollowIR function\n",
    "def predict_labels_FollowIR(questions, options, model, bs=32):\n",
    "    template = \"\"\" Consider the document to be relevant if it shares analogous structure, relationships, or concepts with the query, even if the specific details differ.\"\"\"\n",
    "\n",
    "    scores = []\n",
    "    for bsidx in range(0, len(questions), bs):\n",
    "        cur_questions = questions[bsidx:bsidx+bs]\n",
    "        cur_options = options[bsidx:bsidx+bs]\n",
    "        prompts = [\n",
    "            template.format(query=query, text=text) for (query, text) in zip(cur_questions, cur_options)\n",
    "        ]\n",
    "\n",
    "        # Encode prompts (queries with documents)\n",
    "        prompt_embeddings = model.encode(prompts, batch_size=bs)\n",
    "        # Encode documents separately\n",
    "        doc_embeddings = model.encode(cur_options, batch_size=bs)\n",
    "\n",
    "        # Compute cosine similarities between prompt and document embeddings\n",
    "        similarities = np.sum(prompt_embeddings * doc_embeddings, axis=1)\n",
    "\n",
    "        # Append similarities to scores list\n",
    "        scores.extend(similarities.tolist())\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Life is a Circus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_file_to_qasrl(src, dst):\n",
    "    \"\"\"\n",
    "    Prepare the input for QA-SRL (adding line number, tab and the sentence for every sentence in the text)\n",
    "    \"\"\"\n",
    "    input, output = open(src, 'r'), open(dst, 'w')\n",
    "    for i, line in enumerate(input):\n",
    "        new_line = str(i + 1) + '\\t' + line\n",
    "        output.write(new_line)\n",
    "    input.close()\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analogies_mining.find_mappings import generate_mappings\n",
    "sentence_pair = ([\"Fred built an immense fortune by swindling others, but he lost it all when someone swindled him.\"],\n",
    "                 [\"his shopkeeper always palms off old stock to the customers.\"])\n",
    "generate_mappings(sentence_pair, 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
